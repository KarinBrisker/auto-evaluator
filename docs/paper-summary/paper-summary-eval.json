[
     {"question": "How many parameters does the GLaM model have?",
      "answer": "The largest GLaM model has 1.2T (trillion) parameters."},
     {"question": "How many parameters, layers, and heads does the LaMDA model have?",
      "answer": "The LaMDA model has 137B params, 64 layers, and 128 heads."},
     {"question": "How many tokens is Chinchilla trained on?",
      "answer": "Chinchilla is trained with 1.5T tokens."},
     {"question": "What scaling law does the Chinchilla paper propose?",
      "answer": "For every doubling of model size the number of training tokens should also be doubled."},
     {"question": "How many parameters does the largest Flamingo model have?",
      "answer": "The largest Flamingo model has 80B parameters."},
     {"question": "What tasks is the Gato model capable of?",
      "answer": "The Gato model is capable of tasks including robotics stacking tests, image captioning, and Atari."},
      {"question": "How many parameters does the LLaMA model have?",
      "answer": "The LLaMA model has 65B parameters."}
]